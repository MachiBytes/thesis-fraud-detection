{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118508a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all dependencies\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda3f8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the train_identity.csv file\n",
    "train_identity_df = pd.read_csv('datasets/train_identity.csv')\n",
    "train_identity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc84c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the train_transaction.csv file\n",
    "train_transaction_df = pd.read_csv('datasets/train_transaction.csv')\n",
    "train_transaction_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ce2892",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "## Questions\n",
    "- What does feature engineering contribute to the training of the model?\n",
    "- What are the different types of feature engineering?\n",
    "  - Why should you do a specific type of feature engineering?\n",
    "  - What does it benefit to the performance of the model?\n",
    "  - When should you do specific types of feature engineering?\n",
    "\n",
    "## Answers\n",
    "This process is important because it ensures that garbage in, garbage out doesn't happen. Feature engineering allows us to significantly improve the model performance by refining features. There are various processes in feature engineering.\n",
    "\n",
    "### Processes\n",
    "1. Feature Creation - Create new features\n",
    "   1. Domain-Specific: From industry knowledge like business rules\n",
    "   2. Data-Driven: Derived from recognized patterns\n",
    "   3. Synthetic: From combining existing features\n",
    "2. Feature Transformation - Adjusts features\n",
    "   1. Normalization & Scaling: Adjust the range of features for consistency\n",
    "   2. Encoding: Convert categorical data to numerical data (i.e. one-hot encoding)\n",
    "   3. Mathematical Transformations: Like logarithmic transformations for skewed data\n",
    "3. Feature Extraction - To reduce dimensionality and simplify model\n",
    "   1. Dimensionality Reduction: Reduce features while preserving important information (PCA technique)\n",
    "   2. Aggregation & Combination: Summing/averaging features to simplify the model\n",
    "4. Feature Selection - Choosing a subset of relevant features to use\n",
    "   1. Filter methods: Based on statistical measures like correlation\n",
    "   2. Wrapper methods: Select based on model performance (what?)\n",
    "   3. Embedded methods: Feature selection integrated within model training (what? it's like not manual feature selection but learned feature selection)\n",
    "5. Feature Scaling - Ensuring that all features contribute equally to the model\n",
    "   1. Min-max scaling: Rescales values to a fixed range like 0 to 1\n",
    "   2. Standard scaling: Normalizes to have a mean of 0 and variance of 1\n",
    "      - Note: This is done across all features so that there is no bias towards features with inherently larger numerical values like comparing age to salary.\n",
    "\n",
    "### Steps\n",
    "1. Data Cleaning\n",
    "   - Handling missing values (imputation | replace empty cells with mean, mode, or median values from other cells in the same column)\n",
    "   - Find outliers and handle them\n",
    "     - Replace with statistical number like max or min\n",
    "     - Apply transformations to the feature like log or square root\n",
    "     - Drop the outliers from the dataset\n",
    "     - Note: What is the best approach for handling outliers in our use-case? I feel like we should keep them\n",
    "2. Data Transformation\n",
    "   - Encoding categorical variables\n",
    "     - One-hot encoding: split up the feature into multiple columns like gender will be male and female\n",
    "     - Label encoding: Assign a numerical value for each category label\n",
    "     - Ordinal encoding: Assign numerical value based on the order of the category (if applicable)\n",
    "     - Target encoding: If a category has multiple target values (what does this mean?), take the mean of the values and assign that to the category\n",
    "3. Feature Extraction\n",
    "4. Feature Selection\n",
    "5. Feature Iteration\n",
    "\n",
    "Sources:\n",
    "- [GeekForGeeks - What is Feature Engineering](https://www.geeksforgeeks.org/machine-learning/what-is-feature-engineering/)\n",
    "- [DataCamp - Feature Engineering in Machine Learning](https://www.datacamp.com/tutorial/feature-engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee26f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 20 for transactions\n",
    "print(train_transaction_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c86269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 20 for identity\n",
    "print(train_identity_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c8161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the datasets\n",
    "# Find if there's repeating TransactionIds in train_transaction and train_identity\n",
    "print(f\"Number of unique TransactionIds in train_transaction: {train_transaction_df['TransactionID'].nunique()}\")\n",
    "print(f\"Number of unique TransactionIds in train_identity: {train_identity_df['TransactionID'].nunique()}\")\n",
    "print(f\"Total rows in train_transaction: {len(train_transaction_df)}\")\n",
    "print(f\"Total rows in train_identity: {len(train_identity_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714268b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_transaction_df['isFraud'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7d03d9",
   "metadata": {},
   "source": [
    "# Dataset Exploration\n",
    "\n",
    "[Kaggle discussion regarding dataset columns](https://www.kaggle.com/competitions/ieee-fraud-detection/discussion/101203)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9925c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.merge(train_transaction_df, train_identity_df, on='TransactionID', how='left')\n",
    "print(train_df.shape)\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d82a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_df.columns:\n",
    "    unique_values = train_df[col].nunique()\n",
    "    missing_values = train_df[col].isnull().sum()\n",
    "    if missing_values > 0:\n",
    "        print(f\"Column: {col}, Unique Values: {unique_values}, Missing Values: {missing_values}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
